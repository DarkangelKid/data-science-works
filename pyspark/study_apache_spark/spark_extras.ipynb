{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Big Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import csv, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating.\n"
     ]
    }
   ],
   "source": [
    "# Read mock data\n",
    "mock_data = []\n",
    "with open(\"data/MOCK_DATA.csv\") as mock_file:\n",
    "    mock_data = mock_file.readlines()\n",
    "\n",
    "# randomize index\n",
    "num_range = len(mock_data) - 1\n",
    "sample_items = 100\n",
    "num_iterations = 100000\n",
    "\n",
    "# generate Big Data\n",
    "with open(\"data/big_data.csv\", \"w\") as big_data:\n",
    "    # write header\n",
    "    writer = csv.writer(big_data)\n",
    "    writer.writerow(mock_data[0].strip().split(\",\"))\n",
    "    \n",
    "    # write lines by random sampling 100 items over 1000 lines\n",
    "    next_line = 1\n",
    "    for i in range(0, num_iterations):\n",
    "        lines = []\n",
    "        choices = np.random.choice(num_range, sample_items)\n",
    "        for idx in choices:\n",
    "            line = mock_data[idx].strip().split(\",\")\n",
    "            line[0] = next_line\n",
    "            lines.append(line)\n",
    "            next_line += 1\n",
    "\n",
    "        writer.writerows(lines)\n",
    "\n",
    "print \"Done generating.\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOCK_DATA.csv              sample_kmeans_data.txt\r\n",
      "big_data.csv               sample_lda_libsvm_data.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638M\tdata/big_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs data/big_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store as Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store parquet completed.\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"data/big_data.csv\")\n",
    "\n",
    "df.write.parquet(\"data/big_data_parquet\", mode=\"overwrite\")\n",
    "print \"Store parquet completed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89M\tdata/big_data_parquet\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs data/big_data_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self implement vs Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numer of rows: 10000000\n",
      "Total running time: 11.6682560444 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with open(\"data/big_data.csv\", \"r\") as big_data:\n",
    "    # read csv file\n",
    "    reader = csv.reader(big_data)\n",
    "    header = reader.next()\n",
    "    num_row = 0\n",
    "    for row in reader:\n",
    "        num_row += 1\n",
    "\n",
    "print \"Numer of rows:\", num_row\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print \"Total running time:\", elapsed, \"seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time: 9.40140104294 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "pd_big_data = pd.read_csv(\"data/big_data.csv\")\n",
    "pd_big_data[\"id\"].count()\n",
    "\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "print \"Total running time:\", elapsed, \"seconds\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD vs Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading csv to RDD\n",
    "lines = sc.textFile(\"data/big_data.csv\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "rdd_people = parts.map(lambda p: (p[0], p[1], p[2], p[3], p[4], p[5].strip()))\n",
    "rdd_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading csv to Dataframe\n",
    "df_people = sqlContext.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"data/big_data.csv\")\n",
    "df_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading parquet to Dataframe\n",
    "df_people = sqlContext.read.parquet(\"data/big_data_parquet\")\n",
    "df_people.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "- DISK_ONLY = StorageLevel(True, False, False, False, 1)\n",
    "- DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\n",
    "- MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)\n",
    "- MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)\n",
    "- MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\n",
    "- MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\n",
    "- MEMORY_ONLY = StorageLevel(False, True, False, False, 1)\n",
    "- MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)\n",
    "- MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\n",
    "- MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\n",
    "- OFF_HEAP = StorageLevel(True, True, True, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD cache\n",
    "rdd_people.cache()\n",
    "rdd_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# disk only\n",
    "rdd_people.unpersist()\n",
    "rdd_people.persist(storageLevel=StorageLevel(True, False, False, False, 1))\n",
    "rdd_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe cache\n",
    "df_people.cache()\n",
    "df_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, first_name: string, last_name: string, email: string, gender: string, ip_address: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpersist all\n",
    "rdd_people.unpersist()\n",
    "df_people.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD cache\n",
    "rdd_people.cache()\n",
    "rdd_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe cache\n",
    "df_people.cache()\n",
    "df_people.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_people.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5160545"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_people.filter(lambda x: x[4] == \"Male\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5160545"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people.filter(df_people[\"gender\"] == \"Male\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs Built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|first_name|                mail|\n",
      "+----------+--------------------+\n",
      "|    Linoel|    lcobden7g@hp.com|\n",
      "|    Ripley|rchiplenhp@barnes...|\n",
      "|    Rooney|rchesworth7e@utex...|\n",
      "|    Lovell|lfellgatep8@umich...|\n",
      "|   Cordell|cduplain4a@artist...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_people.filter(df_people[\"gender\"] == \"Male\")\\\n",
    "        .select(df_people[\"first_name\"], df_people[\"email\"]\\\n",
    "        .alias(\"mail\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|first_name|                mail|\n",
      "+----------+--------------------+\n",
      "|    Linoel|    lcobden7g@hp.com|\n",
      "|    Ripley|rchiplenhp@barnes...|\n",
      "|    Rooney|rchesworth7e@utex...|\n",
      "|    Lovell|lfellgatep8@umich...|\n",
      "|   Cordell|cduplain4a@artist...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.registerDataFrameAsTable(df_people, \"tbl_people\")\n",
    "sqlContext.sql(\"\"\"\n",
    "    SELECT first_name, email AS mail\n",
    "    FROM tbl_people\n",
    "    WHERE gender = 'Male'\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=0.0, features=SparseVector(3, {})), Row(label=1.0, features=SparseVector(3, {0: 0.1, 1: 0.1, 2: 0.1}))]\n",
      "Within Set Sum of Squared Errors = 0.12\n",
      "Cluster Centers: \n",
      "[ 0.1  0.1  0.1]\n",
      "[ 9.1  9.1  9.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/sample_kmeans_data.txt\")\n",
    "print dataset.take(2)\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=0.0, features=SparseVector(11, {0: 1.0, 1: 2.0, 2: 6.0, 4: 2.0, 5: 3.0, 6: 1.0, 7: 1.0, 10: 3.0}))]\n",
      "The lower bound on the log likelihood of the entire corpus: -807.523779034\n",
      "The upper bound on perplexity: 3.1058607098\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+------------------------------------------+\n",
      "|topic|termIndices|termWeights                               |\n",
      "+-----+-----------+------------------------------------------+\n",
      "|0    |[4, 7]     |[0.10782279117289076, 0.09748059781126188]|\n",
      "|1    |[1, 6]     |[0.16755680545542245, 0.14746675160950057]|\n",
      "|2    |[1, 3]     |[0.10064404940528088, 0.10044227953257671]|\n",
      "|3    |[1, 3]     |[0.10157580719995081, 0.0997449393879735] |\n",
      "|4    |[9, 10]    |[0.10479880814180582, 0.10207371063193371]|\n",
      "|5    |[8, 5]     |[0.10843493258130431, 0.09701505371078402]|\n",
      "|6    |[8, 5]     |[0.09874157104646761, 0.09654281855423051]|\n",
      "|7    |[9, 4]     |[0.1125248473532763, 0.09755082892584456] |\n",
      "|8    |[5, 4]     |[0.1548707489550002, 0.14842696182913703] |\n",
      "|9    |[3, 10]    |[0.23809170139198915, 0.10412979928333384]|\n",
      "+-----+-----------+------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                      |\n",
      "+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004830687376549748,0.9563377837515517,0.004830654392031491,0.004830691997365709,0.004830667868178218,0.004830690208597917,0.0048307249800948774,0.004830673837555869,0.004922723634004233,0.004924701954070474]     |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.008057899175270963,0.008707406312071347,0.008058025956346778,0.008058095756917977,0.008057857450925326,0.00805788610035481,0.008057863295169719,0.008057922472956825,0.9266708229866527,0.008216220493333638]       |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004199740220179303,0.9620401573867432,0.004199832021596322,0.004199768006758077,0.0041998008830341094,0.0041998188779895085,0.004199829669347301,0.004199780879768142,0.004279810976748579,0.004281461077835567]    |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.0037148977019415986,0.5171649236275588,0.0037149262092321953,0.003714916859012322,0.0037150043988300675,0.003714951767438243,0.0037149809617586303,0.003714886247016027,0.0037853064740571217,0.4530452057531551]   |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.004024698590879546,0.00434854915127394,0.004024734924719693,0.004024704186015809,0.004024729103561261,0.004024699645915288,0.004024728350735192,0.004024732738832953,0.004101250338655413,0.9633771729694108]       |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.0037149196791821672,0.004014144755782942,0.0037150238353502916,0.0037149350730528103,0.003714984985771291,0.0037149612383395674,0.003714972648035956,0.0037150076003473935,0.7875949774197026,0.18238607276443486]  |\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.0038636384437946217,0.4588911758399325,0.0038636648639211735,0.00386366180952517,0.0038637468045085977,0.0038636989406232065,0.003863727465055344,0.003863616485805724,0.00393686849521447,0.5101262008516194]      |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004390951192466292,0.004744392472015055,0.004391008613455925,0.004390986297746747,0.004391002598536641,0.004390967203785097,0.004390995209555504,0.004390994508486955,0.00447440696608868,0.960044294937863]        |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004391055602376864,0.004744679184009837,0.00439115377848033,0.004391124194262692,0.0043910387568340484,0.0043910790167236875,0.004391072093598296,0.004391105932938527,0.7784515959242768,0.18606609551649894]      |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.0033302159688284975,0.9698999591198246,0.0033302386018501102,0.0033302022818941343,0.0033302152189945153,0.003330227969565667,0.00333022302764557,0.0033302283560539235,0.0033935016881488007,0.0033949877671941944]|\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.004199865804144868,0.004538523687990608,0.004199964924904667,0.0041998976831187276,0.004199967375784788,0.004199909609085674,0.004199952755541441,0.004199953028483417,0.004279696097068255,0.9617822690338775]     |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.004830550832388801,0.005219225116360582,0.004830587434608386,0.004830531305324705,0.004830557229897102,0.004830570451296251,0.004830581174355882,0.004830594302879788,0.9560423013264151,0.004924500826473449]      |\n",
      "+-----+---------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/sample_lda_libsvm_data.txt\")\n",
    "print dataset.take(1)\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(2)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(label=0.0, sentence=u'Hi I heard about Spark', words=[u'hi', u'i', u'heard', u'about', u'spark'], rawFeatures=SparseVector(20, {0: 1.0, 5: 1.0, 9: 1.0, 17: 2.0}), features=SparseVector(20, {0: 0.6931, 5: 0.6931, 9: 0.2877, 17: 1.3863}))]\n",
      "The lower bound on the log likelihood of the entire corpus: -37.6225456281\n",
      "The upper bound on perplexity: 4.40556309703\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+-------------------------------------------+\n",
      "|topic|termIndices|termWeights                                |\n",
      "+-----+-----------+-------------------------------------------+\n",
      "|0    |[17, 15]   |[0.05823196670532641, 0.05751424724780337] |\n",
      "|1    |[0, 3]     |[0.062056681029749566, 0.05575172335627538]|\n",
      "+-----+-----------+-------------------------------------------+\n",
      "\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|   topicDistribution|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(20,[0,5,9,17],[1...|(20,[0,5,9,17],[0...|[0.27399693051219...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0.0, \"Hi I heard about Spark\"),\n",
    "    (0.0, \"I wish Java could use case classes\"),\n",
    "    (1.0, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "print rescaledData.take(1)\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=2, maxIter=10)\n",
    "model = lda.fit(rescaledData)\n",
    "\n",
    "ll = model.logLikelihood(rescaledData)\n",
    "lp = model.logPerplexity(rescaledData)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(2)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(rescaledData)\n",
    "transformed.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -44.9571717581\n",
      "The upper bound on perplexity: 4.22351179184\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+-------------------------------------------+\n",
      "|topic|termIndices|termWeights                                |\n",
      "+-----+-----------+-------------------------------------------+\n",
      "|0    |[17, 15]   |[0.05823196670532641, 0.05751424724780337] |\n",
      "|1    |[0, 3]     |[0.062056681029749566, 0.05575172335627538]|\n",
      "+-----+-----------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(20, {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.5754, 5: 0.5754, 6: 0.0, 7: 0.2877, 8: 0.2877, 9: 0.0, 10: 0.0, 11: 0.5754, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 19: 0.0}), label=1.0, topicDistribution=DenseVector([0.3716, 0.6284]))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "\n",
    "ls_row = [Row(label=1.0, features=SparseVector(20, {1: 0.0, 2: 0.0, 3: 0.0, 4: 0.5754, 5: 0.5754, 6: 0.0, 7: 0.2877, 8: 0.2877, 9: 0.0, 10: 0.0, 11: 0.5754, 13: 0.0, 15: 0.0, 16: 0.0, 17: 0.0, 19: 0.0})),\n",
    " Row(label=1.0, features=SparseVector(20, {0: 1.1507, 1: 0.0, 2: 0.0, 3: 0.0, 5: 0.5754, 6: 0.0, 7: 0.5754, 8: 0.5754, 9: 0.0, 10: 0.0, 11: 0.2877, 12: 0.5754, 13: 0.0, 14: 0.5754, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.863, 19: 0.0})),\n",
    " Row(label=1.0, features=SparseVector(20, {0: 0.863, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.5754, 6: 0.0, 9: 0.0, 10: 0.0, 12: 0.2877, 13: 0.0, 14: 0.5754, 15: 0.0, 16: 0.0, 17: 0.0, 18: 0.863, 19: 0.0}))]\n",
    "\n",
    "rescaledData = sc.parallelize(ls_row).toDF()\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=2, maxIter=10)\n",
    "model = lda.fit(rescaledData)\n",
    "\n",
    "ll = model.logLikelihood(rescaledData)\n",
    "lp = model.logPerplexity(rescaledData)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(2)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(rescaledData)\n",
    "transformed.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
